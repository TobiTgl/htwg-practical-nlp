{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Word Embeddings\n",
    "\n",
    "In this notebook, we will apply linear algebra operations using NumPy to find analogies between words manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from htwgnlp.embeddings import WordEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The embeddings we use for this lab are from the [Google News Word2Vec model](https://code.google.com/archive/p/word2vec/). This model was trained on part of the Google News dataset (about 100 billion words). \n",
    "\n",
    "The model contains 300-dimensional vectors for 3 million words and phrases and is about 3.5GB large.\n",
    "\n",
    "For this notebook, we use a small subset of 243 words, which were selected beforehand and are stored in the pickle file `data/embeddings.pkl`.\n",
    "\n",
    "Besides some sample words, it contains mostly capitals and countries. We will use the embeddings to find analogies between words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = WordEmbeddings()\n",
    "embeddings.load_embeddings(\"data/embeddings.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model is loaded, we can take a look at the word representations. \n",
    "\n",
    "We can see that these word embeddings are 300-dimensional vectors.\n",
    "\n",
    "In our case, we only use a small dataset of 243 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"number of features: {len(embeddings.get_embeddings('queen'))}\")\n",
    "print(f\"number of words: {len(embeddings._embeddings.keys())}\")\n",
    "print(embeddings.get_embeddings(\"queen\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operating on word embeddings\n",
    "\n",
    "Word embeddings are the result of machine learning processes and will be part of the input for further processes.\n",
    "\n",
    "Word embeddings are multidimensional arrays, usually with hundreds of attributes that pose a challenge for its interpretation. \n",
    "\n",
    "We can try to visually inspect the word embedding of some words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "words = [\n",
    "    \"oil\",\n",
    "    \"gas\",\n",
    "    \"happy\",\n",
    "    \"sad\",\n",
    "    \"city\",\n",
    "    \"town\",\n",
    "    \"village\",\n",
    "    \"country\",\n",
    "    \"continent\",\n",
    "    \"petroleum\",\n",
    "    \"joyful\",\n",
    "]\n",
    "\n",
    "# Convert each word to its vector representation\n",
    "vectors_2d = np.array([embeddings.get_embeddings(word) for word in words])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "# Select a column for the x and y axes\n",
    "x_axis = 3\n",
    "y_axis = 2\n",
    "\n",
    "# Plot an arrow for each word\n",
    "for word in vectors_2d:\n",
    "    ax.arrow(\n",
    "        0,\n",
    "        0,\n",
    "        word[x_axis],\n",
    "        word[y_axis],\n",
    "        head_width=0.005,\n",
    "        head_length=0.005,\n",
    "        fc=\"r\",\n",
    "        ec=\"r\",\n",
    "        width=1e-5,\n",
    "    )\n",
    "\n",
    "# Plot a dot for each word\n",
    "ax.scatter(vectors_2d[:, x_axis], vectors_2d[:, y_axis])\n",
    "\n",
    "# Add the word label over each dot in the scatter plot\n",
    "for i in range(0, len(words)):\n",
    "    ax.annotate(words[i], (vectors_2d[i, x_axis], vectors_2d[i, y_axis]))\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that similar words like 'village' and 'town' or 'petroleum', 'oil', and 'gas' tend to point in the same direction. \n",
    "\n",
    "Also, note that 'sad' and 'happy' looks close to each other; however, the vectors point in opposite directions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word distance\n",
    "\n",
    "Now we plot the words 'sad', 'happy', 'town', and 'village' and display the vector from 'village' to 'town' and the vector from 'sad' to 'happy'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [\"sad\", \"happy\", \"town\", \"village\"]\n",
    "\n",
    "# Convert each word to its vector representation\n",
    "vectors_2d = np.array([embeddings.get_embeddings(word) for word in words])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "# Select a column for the x and y axes\n",
    "x_axis = 3\n",
    "y_axis = 2\n",
    "\n",
    "# Print an arrow for each word\n",
    "for word in vectors_2d:\n",
    "    ax.arrow(\n",
    "        0,\n",
    "        0,\n",
    "        word[x_axis],\n",
    "        word[y_axis],\n",
    "        head_width=0.0005,\n",
    "        head_length=0.0005,\n",
    "        fc=\"r\",\n",
    "        ec=\"r\",\n",
    "        width=1e-5,\n",
    "    )\n",
    "\n",
    "# plot the vector difference between village and town\n",
    "village = embeddings.get_embeddings(\"village\")\n",
    "town = embeddings.get_embeddings(\"town\")\n",
    "diff = town - village\n",
    "ax.arrow(\n",
    "    village[x_axis],\n",
    "    village[y_axis],\n",
    "    diff[x_axis],\n",
    "    diff[y_axis],\n",
    "    fc=\"b\",\n",
    "    ec=\"b\",\n",
    "    width=1e-5,\n",
    ")\n",
    "\n",
    "# plot the vector difference between village and town\n",
    "sad = embeddings.get_embeddings(\"sad\")\n",
    "happy = embeddings.get_embeddings(\"happy\")\n",
    "diff = happy - sad\n",
    "ax.arrow(\n",
    "    sad[x_axis], sad[y_axis], diff[x_axis], diff[y_axis], fc=\"b\", ec=\"b\", width=1e-5\n",
    ")\n",
    "\n",
    "# Plot a dot for each word\n",
    "ax.scatter(vectors_2d[:, x_axis], vectors_2d[:, y_axis])\n",
    "\n",
    "# Add the word label over each dot in the scatter plot\n",
    "for i in range(0, len(words)):\n",
    "    ax.annotate(words[i], (vectors_2d[i, x_axis], vectors_2d[i, y_axis]))\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting capitals\n",
    "\n",
    "Now, applying vector addition or substraction, one can create a vector representation for a new word. For example, we can say that the vector difference between 'France' and 'Paris' represents the concept of the capital of a country.\n",
    "\n",
    "We can move from the city of Madrid in the direction of the concept of capital, and obtain something close to the corresponding country to which Madrid is the capital.\n",
    "\n",
    "For this, recap vector subtraction:\n",
    "\n",
    "![Vector substraction](https://upload.wikimedia.org/wikipedia/commons/thumb/2/24/Vector_subtraction.svg/206px-Vector_subtraction.svg.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capital_to_country = embeddings.get_embeddings(\"France\") - embeddings.get_embeddings(\n",
    "    \"Paris\"\n",
    ")\n",
    "\n",
    "print(capital_to_country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_country = embeddings.get_embeddings(\"Madrid\") + capital_to_country\n",
    "print(predicted_country)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that we do not end up exactly in the corresponding country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = predicted_country - embeddings.get_embeddings(\"Spain\")\n",
    "print(diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we have to look for the closest words in the embedding that matches the predicted country. \n",
    "\n",
    "If the word embedding works as expected, the most similar word must be 'Spain'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings.find_closest_word(predicted_country, metric=\"euclidean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings.euclidean_distance(predicted_country).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if cosine similarity also works as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings.find_closest_word(predicted_country, metric=\"cosine\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting other Countries\n",
    "\n",
    "Let's play around a little bit, and see if we also end up in Spain when we start from the capital of another countries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings.find_closest_word(\n",
    "    embeddings.get_embeddings(\"Italy\")\n",
    "    - embeddings.get_embeddings(\"Rome\")\n",
    "    + embeddings.get_embeddings(\"Madrid\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try to predict the country from the capital of some another countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countr_of_berlin = embeddings.get_embeddings(\"Berlin\") + capital_to_country\n",
    "countr_of_beijing = embeddings.get_embeddings(\"Beijing\") + capital_to_country\n",
    "\n",
    "print(f\"Berlin is the capital of: {embeddings.find_closest_word(countr_of_berlin)}\")\n",
    "print(f\"Beijing is the capital of: {embeddings.find_closest_word(countr_of_beijing)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And test the prediction with cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"Berlin is the capital of: {embeddings.find_closest_word(countr_of_berlin, metric='cosine')}\"\n",
    ")\n",
    "print(\n",
    "    f\"Beijing is the capital of: {embeddings.find_closest_word(countr_of_beijing, metric='cosine')}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use the `get_most_similar_words` function and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings.get_most_similar_words(\"Spain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = \"Spain\"\n",
    "print(\n",
    "    f\"Most similar words to '{word}' by euclidean: {embeddings.get_most_similar_words(word, metric='euclidean')}\"\n",
    ")\n",
    "print(\n",
    "    f\"Most similar words to '{word}' by cosine: {embeddings.get_most_similar_words(word, metric='cosine')}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that Spain itself is not returned, as we want to find similar words, not the same word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = \"Berlin\"\n",
    "print(\n",
    "    f\"Most similar words to '{word}' by euclidean: {embeddings.get_most_similar_words(word, metric='euclidean')}\"\n",
    ")\n",
    "print(\n",
    "    f\"Most similar words to '{word}' by cosine: {embeddings.get_most_similar_words(word, metric='cosine')}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = \"happy\"\n",
    "print(\n",
    "    f\"Most similar words to '{word}' by euclidean: {embeddings.get_most_similar_words(word, metric='euclidean')}\"\n",
    ")\n",
    "print(\n",
    "    f\"Most similar words to '{word}' by cosine: {embeddings.get_most_similar_words(word, metric='cosine')}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "If we have word embeddings available, we can use simple vector operations to find relationships between words. Using this technique, we can already find some interesting relationships between words."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
