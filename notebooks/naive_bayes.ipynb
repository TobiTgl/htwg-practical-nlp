{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis with Naive Bayes\n",
    "\n",
    "In this notebook, we will explore the use of Naive Bayes for sentiment analysis.\n",
    "\n",
    "It is essentially the same task as in the previous assignment, except we use Naive Bayes this time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "We will use the same dataset as in the previous assignment, that is the NLTK tweets dataset.\n",
    "\n",
    "Also we will do the same train/test split as in the previous assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import twitter_samples\n",
    "\n",
    "postive_tweets = twitter_samples.strings(\"positive_tweets.json\")\n",
    "negative_tweets = twitter_samples.strings(\"negative_tweets.json\")\n",
    "n_samples = len(postive_tweets) + len(negative_tweets)\n",
    "n_pos = len(postive_tweets)\n",
    "n_neg = len(negative_tweets)\n",
    "\n",
    "print(\"Total number of tweets: \", n_samples)\n",
    "print(\"Number of positive tweets: \", n_pos)\n",
    "print(\"Number of negative tweets: \", n_neg)\n",
    "\n",
    "n_train = int(n_samples * 0.8)\n",
    "n_test = n_samples - n_train\n",
    "\n",
    "print(\"Number of training samples: \", n_train)\n",
    "print(\"Number of test samples: \", n_test)\n",
    "\n",
    "n = int(n_train / 2)\n",
    "\n",
    "# training data\n",
    "train_data_pos = postive_tweets[:n]\n",
    "train_data_neg = negative_tweets[:n]\n",
    "print(f\"train_data_pos: {len(train_data_pos)}\")\n",
    "print(f\"train_data_neg: {len(train_data_neg)}\")\n",
    "\n",
    "# test data\n",
    "test_data_pos = postive_tweets[n:]\n",
    "test_data_neg = negative_tweets[n:]\n",
    "print(f\"test_data_pos: {len(test_data_pos)}\")\n",
    "print(f\"test_data_neg: {len(test_data_neg)}\")\n",
    "\n",
    "# build train and test datasets\n",
    "train_data = train_data_pos + train_data_neg\n",
    "test_data = test_data_pos + test_data_neg\n",
    "print(f\"train_data: {len(train_data)}\")\n",
    "print(f\"test_data: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# create labels\n",
    "y_train = np.append(\n",
    "    np.ones((len(train_data_pos), 1)), np.zeros((len(train_data_neg), 1)), axis=0\n",
    ")\n",
    "y_test = np.append(\n",
    "    np.ones((len(test_data_pos), 1)), np.zeros((len(test_data_neg), 1)), axis=0\n",
    ")\n",
    "\n",
    "print(\"y_train shape: \", y_train.shape)\n",
    "print(\"y_test shape: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "We will reuse our preprocessing pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from htwgnlp.preprocessing import TweetProcessor\n",
    "\n",
    "processor = TweetProcessor()\n",
    "train_data_processed = [processor.process_tweet(tweet) for tweet in train_data]\n",
    "train_data_processed[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "For training, the goal is to find the word probabilities for each class.\n",
    "\n",
    "Also we need the log ratio of the probabilities, which are calculated from the word probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from htwgnlp.naive_bayes import NaiveBayes\n",
    "\n",
    "model = NaiveBayes()\n",
    "\n",
    "model.fit(train_data_processed, y_train)\n",
    "model.word_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.log_ratios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "For testing, we need to make sure to apply the same preprocessing pipeline as for training.\n",
    "\n",
    "Then we can calculate the log ratio of the probabilities for each class.\n",
    "\n",
    "This is done by the `predict` function, which returns the predicted class label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_processed = [processor.process_tweet(tweet) for tweet in test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(test_data_processed)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "We can observe that we achieve a relatively high accuracy of 99.65% on the test set.\n",
    "\n",
    "```\n",
    "# expected output\n",
    "Accuracy: 0.9965\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Accuracy: {(y_pred == y_test).mean() * 100}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can try to predict our own tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = \"Konstanz is a great place to live!\"\n",
    "x_i = [processor.process_tweet(tweet)]\n",
    "print(f\"tweet: {x_i[0]}\")\n",
    "print(f\"prediction: {model.predict(x_i)[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Analysis\n",
    "\n",
    "Finally, we can check the error cases to see where our model fails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_cases = np.nonzero((y_pred.flatten() != y_test.flatten()))[0]\n",
    "y_prob = model.predict_prob(test_data_processed)\n",
    "\n",
    "for i in error_cases:\n",
    "    print(\n",
    "        f\"sample: {i:>4}, predicted class: {y_pred[i]}, actual class: {y_test[i]} log likelihood: {y_prob[i].item():7.4f}, tweet: {test_data[i]}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To better understand our classifier, we can check which words have the most impact on the sentiment of the review.\n",
    "\n",
    "We can use the log ratios of the conditional probabilities to find the words that are most indicative of a positive or negative tweet.\n",
    "\n",
    "Remember from the lecture that a value greater than 0 means that the word is more likely to appear in a positive tweet, and a value less than 0 means that the word is more likely to appear in a negative tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.log_ratios.sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the counts may give us a better intuition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = model.df_freqs.copy()\n",
    "\n",
    "df[\"ratio\"] = (df[1] + 1) / (df[0] + 1)\n",
    "df.sort_values(by=\"ratio\", ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by=\"ratio\").head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "The Naive Bayes classifier is a simple but powerful classifier that works well on text classification problems. \n",
    "\n",
    "It makes the assumption that the features are conditionally independent given the class, which is not true in general, but it still performs well in practice.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
